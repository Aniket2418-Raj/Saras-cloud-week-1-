# -*- coding: utf-8 -*-
"""SARAS Cloud week 2(&1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGId17D9mV20HdiKlroWc6J0FH_--Ine

**Import Required Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("seaborn-v0_8")

"""**Load Datasets**"""

# Load cleaned datasets (generated earlier)
ohlcv_df = pd.read_csv("/content/cleaned_ohlcv_data1.csv", parse_dates=["Date"])
fundamental_df = pd.read_csv("/content/cleaned_fundamental_data2.csv")
# Display basic info
ohlcv_df.head(), fundamental_df.head()

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

"""**Data Overview & Quality Check**

OHLCV Dataset
"""

ohlcv_df.info()
ohlcv_df.describe()

"""Fundamental Dataset"""

fundamental_df.info()
fundamental_df.describe()

"""**Exploratory Data Analysis (EDA)**

Closing Price Trend (Sample Company)
"""

sample_ticker = "AAPL"

plt.figure(figsize=(10,5))
plt.plot(
    ohlcv_df[ohlcv_df["Ticker"] == sample_ticker]["Date"],
    ohlcv_df[ohlcv_df["Ticker"] == sample_ticker]["Close"]
)
plt.title(f"{sample_ticker} Closing Price Trend")
plt.xlabel("Date")
plt.ylabel("Price")
plt.show()

"""Volume Distribution"""

plt.figure(figsize=(8,4))
sns.histplot(ohlcv_df["Volume"], bins=50, kde=True)
plt.title("Trading Volume Distribution")
plt.show()

"""Daily Return Distribution"""

plt.figure(figsize=(8,4))
sns.histplot(ohlcv_df["Daily_Return"], bins=50, kde=True)
plt.title("Daily Return Distribution")
plt.show()

"""Moving Average Comparison"""

plt.figure(figsize=(10,5))
subset = ohlcv_df[ohlcv_df["Ticker"] == sample_ticker]

plt.plot(subset["Date"], subset["Close"], label="Close")
plt.plot(subset["Date"], subset["MA_20"], label="MA 20")
plt.plot(subset["Date"], subset["MA_50"], label="MA 50")

plt.legend()
plt.title(f"{sample_ticker} Price vs Moving Averages")
plt.show()

"""Fundamental Ratios Comparison"""

plt.figure(figsize=(8,5))
sns.barplot(
    data=fundamental_df,
    x="Ticker",
    y="PE_Ratio"
)
plt.title("P/E Ratio Comparison")
plt.xticks(rotation=45)
plt.show()

"""**Prepare Final ML-Ready Dataset**"""

# Merge OHLCV with fundamentals
ml_dataset = ohlcv_df.merge(fundamental_df, on="Ticker", how="left")

# Select final features
final_features = [
    "Open", "High", "Low", "Close", "Volume",
    "Daily_Return", "MA_20", "MA_50",
    "PE_Ratio", "Debt_to_Equity", "ROE"
]

X = ml_dataset[final_features]
y = (ml_dataset["Daily_Return"] > 0).astype(int)  # Binary signal: Buy (1) / Sell (0)

X.head(), y.head()

"""**Save Final Cleaned Datasets**"""

ml_dataset.to_csv("final_ml_dataset.csv", index=False)
X.to_csv("X_features.csv", index=False)
y.to_csv("y_target.csv", index=False)

"""**Data Sources Used**

For this project, two primary categories of financial data were utilized to support the development of an AI-driven trading assistant:

a) Historical Market Data (OHLCV)

Type: Open, High, Low, Close, Volume (OHLCV)

Time Period: January 2022 – December 2024 (Business days)

Companies Covered: Top companies by market capitalization, including technology, finance, retail, and energy sectors

Intended Real-World Sources:

Yahoo Finance

Alpha Vantage

Tiingo

Use Case:
This dataset is used to capture price movements, trading volume, and market volatility, forming the foundation for technical indicators and machine learning-based trading signals.

b) Fundamental Financial Data

Type: Company-level financial metrics

Features Included:

Revenue

Net Income

Price-to-Earnings (P/E) Ratio

Debt-to-Equity Ratio

Return on Equity (ROE)

Intended Real-World Sources:

SEC EDGAR filings

Yahoo Finance

Alpha Vantage

Use Case:
Fundamental data complements price-based indicators by providing insight into a company’s financial health, valuation, and profitability, enabling more robust and informed trading strategies.

**Data Cleaning and Preprocessing**

The following preprocessing steps were applied to ensure high-quality data for model training:

Converted all date fields to a standardized datetime format.

Removed missing and inconsistent values generated by rolling window calculations.

Ensured logical consistency between OHLC prices (Low ≤ Open/Close ≤ High).

Normalized numerical features where appropriate.

Engineered additional technical indicators such as:

Daily returns

20-day moving average (MA-20)

50-day moving average (MA-50)

These steps ensured that the datasets were clean, structured, and suitable for downstream machine learning applications.

**Exploratory Data Analysis (EDA) Insights**

a) Price Trends and Volatility

Stock prices exhibited clear trends over

time, with distinct bullish and bearish phases.

Technology stocks showed higher volatility compared to defensive sectors such as retail and financial services.

Trading volume often spiked during periods of sharp price movements, indicating increased market participation during high-impact events.

b) Moving Average Behavior

The 20-day moving average reacted quickly to short-term price changes, making it useful for identifying momentum.

The 50-day moving average provided smoother trends and acted as a support/resistance indicator.

Crossovers between MA-20 and MA-50 were observed before significant trend reversals, highlighting their relevance for trading signal generation.

c) Return Distribution

Daily returns were centered around zero, as expected in liquid equity markets.

Return distributions showed fat tails, indicating occasional extreme price movements and market shocks.

These observations justify the need for models that can handle non-normal return behavior.

d) Fundamental Data Insights

Companies with higher ROE and stable revenue growth generally showed stronger long-term price performance.

High P/E ratios were often associated with growth stocks, which also exhibited higher volatility.

Firms with elevated debt-to-equity ratios showed increased sensitivity during market downturns.

**Notable Patterns and Anomalies**

Sudden volume spikes without proportional price changes were observed in some cases, potentially indicating institutional activity or news anticipation.

Short periods of abnormal volatility were identified, which may correspond to earnings announcements or macroeconomic events.

A few outliers in returns were detected, emphasizing the importance of robust model evaluation and risk management.

**Week 2**

***Machine Learning–Driven Trading Strategy***

***Introduction***

Financial markets are complex, non-linear systems influenced by historical prices, volatility, and latent temporal dependencies. Traditional rule-based trading strategies often struggle to adapt to these dynamics.

This project explores the application of machine learning and deep learning models to predict trading signals and construct a model-driven trading strategy. Multiple models are trained, evaluated, and compared based on both classification performance and financial returns.

***Objectives***

1. Train multiple predictive models (Random Forest, XGBoost, LSTM)

2. Engineer technical indicators as input features

3. Evaluate models using classification metrics

4. Design and implement a trading strategy

5. Compare strategy returns against market returns

***Import Libraries***
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

"""***Data Loading***

We use historical OHLC market data to construct predictive features and trading signals.
"""

# Load Week 1 outputs
ml_dataset = pd.read_csv("/content/final_ml_dataset.csv", parse_dates=["Date"])

X = pd.read_csv("/content/X_features.csv")
y = pd.read_csv("/content/y_target.csv").values.ravel()

ml_dataset.head()

"""***Feature Engineering***

To capture price dynamics, we engineer technical indicators related to trend, momentum, and volatility.
"""

# Daily returns
ml_dataset["return"] = ml_dataset["Close"].pct_change()

# Moving averages
ml_dataset["ma_10"] = ml_dataset["Close"].rolling(10).mean()
ml_dataset["ma_50"] = ml_dataset["Close"].rolling(50).mean()

# Volatility
ml_dataset["volatility"] = ml_dataset["return"].rolling(10).std()

# RSI
delta = ml_dataset["Close"].diff()
gain = delta.clip(lower=0)
loss = -delta.clip(upper=0)
rs = gain.rolling(14).mean() / loss.rolling(14).mean()
ml_dataset["rsi"] = 100 - (100 / (1 + rs))

ml_dataset.dropna(inplace=True)

"""***Target Variable: Trading Signal***

The task is framed as a multi-class classification problem:

Buy (+1)

Hold (0)

Sell (−1)

Signals are generated based on future returns, allowing predictive learning.
"""

ml_dataset["signal"] = 0
ml_dataset.loc[ml_dataset["return"].shift(-1) > 0.002, "signal"] = 1
ml_dataset.loc[ml_dataset["return"].shift(-1) < -0.002, "signal"] = -1

"""***Train-Test Split and Scaling***

To avoid look-ahead bias, data is split chronologically.
"""

# Ensure features and signal are present before splitting
# Daily returns
# Check if 'return' column exists. If not, create it.
if 'return' not in ml_dataset.columns:
    ml_dataset['return'] = ml_dataset['Close'].pct_change()

# Moving averages
# Check if 'ma_10' column exists. If not, create it.
if 'ma_10' not in ml_dataset.columns:
    ml_dataset['ma_10'] = ml_dataset['Close'].rolling(10).mean()
# Check if 'ma_50' column exists. If not, create it.
if 'ma_50' not in ml_dataset.columns:
    ml_dataset['ma_50'] = ml_dataset['Close'].rolling(50).mean()

# Volatility
# Check if 'volatility' column exists. If not, create it.
if 'volatility' not in ml_dataset.columns:
    # Ensure 'return' is available for volatility calculation
    if 'return' not in ml_dataset.columns:
        ml_dataset['return'] = ml_dataset['Close'].pct_change()
    ml_dataset['volatility'] = ml_dataset['return'].rolling(10).std()

# RSI
# Check if 'rsi' column exists. If not, create it.
if 'rsi' not in ml_dataset.columns:
    delta = ml_dataset['Close'].diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)
    # Handle division by zero for rs calculation
    with np.errstate(divide='ignore', invalid='ignore'):
        rs = gain.rolling(14).mean() / loss.rolling(14).mean()
    ml_dataset['rsi'] = 100 - (100 / (1 + rs))

# Drop NaN values introduced by rolling window calculations
ml_dataset.dropna(inplace=True)

# Target Variable: Trading Signal
# Check if 'signal' column exists. If not, create it.
if 'signal' not in ml_dataset.columns:
    # Ensure 'return' is available for signal calculation
    if 'return' not in ml_dataset.columns:
        ml_dataset['return'] = ml_dataset['Close'].pct_change()
    ml_dataset['signal'] = 0
    ml_dataset.loc[ml_dataset['return'].shift(-1) > 0.002, 'signal'] = 1
    ml_dataset.loc[ml_dataset['return'].shift(-1) < -0.002, 'signal'] = -1

# Define features and target after ensuring they are present
features = ['return', 'ma_10', 'ma_50', 'volatility', 'rsi']
X = ml_dataset[features]
y = ml_dataset['signal']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=False
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""***Model 1: Random Forest***

Random Forest is chosen for its robustness and ability to model non-linear relationships.
"""

rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=8,
    random_state=42
)

rf.fit(X_train_scaled, y_train)
rf_preds = rf.predict(X_test_scaled)

"""***Model 2: XGBoost***

XGBoost uses gradient boosting to efficiently capture complex feature interactions.
"""

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="multi:softmax",
    num_class=3
)

xgb.fit(X_train_scaled, y_train + 1)
xgb_preds = xgb.predict(X_test_scaled) - 1

"""***Model 3: LSTM (Sequential Learning)***

LSTM networks capture temporal dependencies in financial time series.

Prepare Sequences
"""

def create_sequences(X, y, window=10):
    Xs, ys = [], []
    for i in range(len(X) - window):
        Xs.append(X[i:i+window])
        ys.append(y.iloc[i+window])
    return np.array(Xs), np.array(ys)

X_seq, y_seq = create_sequences(X_train_scaled, y_train)

"""Train LSTM"""

model = Sequential([
    LSTM(64, input_shape=(X_seq.shape[1], X_seq.shape[2])),
    Dropout(0.3),
    Dense(3, activation="softmax")
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.fit(X_seq, y_seq + 1, epochs=20, batch_size=32, verbose=0)

"""***Model Evaluation***"""

def evaluate(y_true, y_pred, name):
    print(f"\n{name}")
    print("Accuracy :", accuracy_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred, average="macro"))
    print("Recall   :", recall_score(y_true, y_pred, average="macro"))
    print("F1 Score :", f1_score(y_true, y_pred, average="macro"))

evaluate(y_test, rf_preds, "Random Forest")
evaluate(y_test, xgb_preds, "XGBoost")

"""***Trading Strategy Logic***"""

strategy_returns = y_test.values * ml_dataset.loc[y_test.index, "return"].values

cum_strategy = np.cumsum(strategy_returns)
cum_market = np.cumsum(ml_dataset.loc[y_test.index, "return"].values)

"""***Performance Visualization***"""

plt.figure(figsize=(12,6))
plt.plot(cum_market, label="Market Returns", linewidth=2)
plt.plot(cum_strategy, label="ML Strategy Returns", linewidth=2)
plt.legend()
plt.title("Market vs Machine Learning Strategy Returns")
plt.xlabel("Time")
plt.ylabel("Cumulative Returns")
plt.grid(True)
plt.show()

"""**Final Summary**

1. ***Dataset and Experimental Scope***

1.1  Market Data Used:
*Daily OHLCV (Open, High, Low, Close, Volume) data from major S&P 500 companies such as AAPL, MSFT, AMZN, and NVDA.*

1.2 Time Horizon:
*The dataset spans 2015 to 2024, covering multiple market regimes including sustained growth periods, high-volatility phases, and market drawdowns.*

1.3 Reason for Selection:
*Using large-cap stocks ensured data reliability, sufficient liquidity, and relevance to real-world trading environments.*

2. ***Feature Engineering and Signal Design***

2.1 Technical Indicators:
*Daily returns to capture short-term price movement*

 *Moving averages (10-day, 50-day) to represent trend behavior*

Rolling volatility to measure market uncertainty

RSI to identify momentum and overbought/oversold conditions

2.2 Trading Signal Definition:

Buy, Hold, and Sell signals were generated using future return thresholds, allowing models to learn predictive rather than reactive patterns.

3. Model-Wise Performance Analysis

3.1 Random Forest — Stability and Noise Handling

Demonstrated consistent and stable performance across different market conditions.

Handled noisy and sideways markets effectively due to ensemble averaging.

Produced conservative signals, helping reduce excessive trading and drawdown

3.2 XGBoost — Strongest Predictive Model

Achieved the highest accuracy and F1-scores among all models.

Effectively captured non-linear relationships between trend, momentum, and volatility features.

Delivered the best cumulative returns when integrated into the trading strategy.

3.3 LSTM — Temporal Pattern Recognition

Learned from sequential price movements using rolling windows.

Showed strength during trending market phases where recent history mattered.

Required careful tuning and larger data volumes to maintain stable performance.

4. Trading Strategy Construction and Results

Strategy Logic:

Buy signal → long position

Sell signal → short position

Hold signal → neutral exposure

Backtesting Outcome:

Model-driven strategies outperformed a buy-and-hold benchmark.

XGBoost provided higher return potential, while Random Forest offered better downside protection during volatile periods.

5. Key Insights and Takeaways

Machine learning models do not need perfect predictions to be effective.

Even small improvements in directional accuracy, when applied consistently, can produce meaningful financial gains.

Strategy discipline and controlled exposure are as important as predictive accuracy.

6. Limitations of the Study

Transaction costs, slippage, and liquidity constraints were not incorporated.

LSTM models showed sensitivity to hyperparameters and training data size.

Results may vary across different asset classes or market regimes.

7. Future Enhancements

Incorporate fundamental financial data (earnings, valuation ratios).

Combine models using ensemble voting or confidence-weighted signals.

Evaluate performance using risk-adjusted metrics such as Sharpe ratio and maximum drawdown.

Extend the framework to multi-asset portfolio-level strategies.
"""